# WEEK 2

<img align = "left" src="./assets/image-20221123201741188.png" alt="image-20221123201741188" style="zoom: 67%;" />






[TOC]
# 多特征向量（入门）-Multiple features

**vector**-向量

<img src="./assets/image-20221123203205812.png" alt="image-20221123203205812" style="zoom:67%;" />

当我们考虑多个影响时，可以想象下面的这个形式来表述

<img align="left" src="./assets/image-20221123203356992.png" alt="image-20221123203356992" style="zoom:50%;" />

**dot product**-点乘

<img src="./assets/image-20221123203902787.png" alt="image-20221123203902787" style="zoom: 50%;" />

> 多元线性回归-Multiple linear regression
## 矢量处理-向量运算

**vectorization**-矢量化，可以更容易处理你的数据

> Python的索引是从零开始的

- 手动计算点乘<img src="./assets/image-20221123204434599.png" alt="image-20221123204434599" style="zoom: 33%;" />

- 循环写法计算求和<img src="./assets/image-20221123204602412.png" alt="image-20221123204602412" style="zoom:33%;" />
- 向量的点乘<img src="./assets/image-20221123204659930.png" alt="image-20221123204659930" style="zoom:33%;" />

<img src="./assets/image-20221123204723148.png" alt="image-20221123204723148" style="zoom: 67%;" />

向量化可以更加方便运算与阅读，而且计算机的向量化是并行计算（硬件），相比于之前的会快很多。

<img src="./assets/image-20221123205109848.png" alt="image-20221123205109848" style="zoom:80%;" />

![image-20221123205337862](image-20221123205337862.png)

>**向量化处理数据**-为了后续改善多元回归梯度下降

## 多元线性回归的梯度下降-Gradient descent for multiple linear regression

>number-b，b不是向量，是标量。

<img src="./assets/image-20221123205845738.png" alt="image-20221123205845738" style="zoom:80%;" />



![image-20221123210239600](image-20221123210239600.png)


$$\frac{\partial}{\partial w_{j}}J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}\left(f_{\overrightarrow{W},b}\left(\vec{x}^{(i)}\right)-y^{(i)}\right)x_{1}^{(i)}$$

$$\frac{\partial}{\partial w_{j}}J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})$$

## 正规方程-**Normal equation**

 [详解正规方程（Normal Equation） - 知乎 ](https://zhuanlan.zhihu.com/p/60719445)

（用到了很多的矩阵求导公式,可以用来检测结果）
$$\theta=(X^TX)^{-1}X^TY$$

优点： 
无需迭代

缺点：
只能在多元线性回归中用到
如果数据量过大，会很慢

 <img align="middle" src="./assets/image-20221123210655127.png" alt="image-20221123210655127" style="zoom:25%;" />

# 有关特征值与参数调节工程

## 特征放缩-Feature scaling

>  有的参数或者训练数据差异很大

<img src="./assets/image-20221124163019560.png" alt="image-20221124163019560" style="zoom:67%;" />

>如何合理的取值（大小关系上）

<img src="./assets/image-20221124163226331.png" alt="image-20221124163226331" style="zoom:67%;" />

<img src="./assets/image-20221124163351938.png" alt="image-20221124163351938" style="zoom:67%;" />

缩放后更加精确美观的标出我们的取值范围区域

### 均值归一化-**Mean Normalization**

<img src="./assets/image-20221124163801674.png" alt="image-20221124163801674" style="zoom:50%;" />


### Z-Score标准化-**Z-Score Normalization**

<img src="./assets/image-20221124163954440.png" alt="image-20221124163954440" style="zoom:50%;" />

### 总结
尽量使得所有的变量所属区间长度相近，特征放缩可以使梯度下降更快

<img align="left" src="./assets/image-20221124164625125.png" alt="image-20221124164625125" style="zoom:67%;" />

## 检验收敛性-Check the convergence

### 怎么看收敛性

一般都是要单减，否则说明有bug或者$\alpha$选的不好

<img src="./assets/image-20221124165359209.png" alt="image-20221124165359209" style="zoom:67%;" />

> 自动算法像是规定一个误差，减少基本趋于0时跳出。

### 如何选取学习率


学习率$\alpha$

![image-20221124165833409](image-20221124165833409.png)

多带入一些$\alpha$的值进行测试（调参）

<img src="./assets/image-20221124170005112.png" alt="image-20221124170005112" style="zoom:67%;" />

# 特征工程-Feature Engineering

<img src="./assets/image-20221124170740687.png" alt="image-20221124170740687" style="zoom:67%;" />

用知识构造一个新的特征（变量）能够反映更多的特性，还可以使得整体更加拟合。

# 多项式回归-Polynomial Regression

<img src="./assets/image-20221124171128156.png" alt="image-20221124171128156" style="zoom:67%;" />

此时，特征放缩-**feature scaling**尤为重要。那么该如何选，以后的课程会分析各种情况下的优缺点。
