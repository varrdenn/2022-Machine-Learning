# WEEK 8

[TOC]

# 前言

无监督学习：聚类，异常检测anomaly detection

推荐系统-商业，广告

强化学习：电子游戏

![image-20221201202019878](image-20221201202019878.png)

# 聚类-clustering

## 聚类的介绍

非监督学习的一种，之前也提过，但是不给y的标签了，让机器自己分类

<img src="./assets/image-20221201202140665.png" alt="image-20221201202140665" style="zoom:33%;" /><img src="./assets/image-20221201202152579.png" alt="image-20221201202152579" style="zoom:33%;" />

让机器自己寻找结构，可以将其分成簇-**clusters**

![image-20221201202441313](image-20221201202441313.png)

宇宙中的星系

# k-means均值聚类算法

举例：

找到簇质心-**cluster centroids**

**Step0**.随机找两个点-簇质心

**Step1**.历遍所有点，**按照距离远近的划分为两个簇**
![image-20221201202802696](image-20221201202802696.png)

**Step2**.移动簇质心，然后重复**Step1**

![image-20221201202928656](image-20221201202928656.png)

![image-20221201203052983](image-20221201203052983.png)

然后再依次重复下去

![image-20221201203132804](image-20221201203132804.png)

最后到稳定收敛就可以了

实际上就是每个簇应该有一个质心，只不过是通过这种方式来从外围步步迭代到质心

## k-means代码实现

质心的维数与数据集每个数据的维数相同，k个就代表你刚开始放了k个质心点

- Step 1

$$
这里就是要找离x^{(i)}最近的k值(u_k点)，min_k{||x^{(i)}-u_k||}，这里是二范数(距离空间的距离)
$$

- Step 2

  计算质心位置

![image-20221203192858865](image-20221203192858865.png)

> 边界情况：如果出现0个点距离近，那么可以去掉这个点，k=k-1，或者直接再随机分点

其实只要不是同一个点，就会自动分成两个簇（不然全在垂直平分线上）

![image-20221203193345304](image-20221203193345304.png)

> 即使区分不那么明显，也可以起到一定的价值作用

## 代价函数-distortion function

![image-20221205142717519](image-20221205142717519.png)

**distortion**-失真or畸变，**index**-返回值函数

![image-20221205143316219](image-20221205143316219.png)

> 中间那一项算的是中点（质心）（余弦定理）

![image-20221205143552202](image-20221205143552202.png)
> 至于这里为什么要找质心，我们可以稍微一想

我们假设簇中心坐标为
$$p_0 = (x_1,x_2,x_3,\dots ,x_n)$$
假设所有的样本点为
$$\{p_m = (x_1^{(m)},x_2^{(m)},x_3^{(m)},\dots,x_n^{(m)})\}$$
那么求最短距离就是
$$min \sum_{i=1}^m||p_0 - p_i||^2$$
展开来，就有
$$\sum_{i=1}^m||p_0 - p_i||^2 = \sum_{i=1}^m[(x_1-x_1^{(i)})^2+(x_2-x_2^{(i)})^2+(x_3-x_3^{(i)})^2+\dots+(x_n-x_n^{(i)})^2]$$
$$=\sum_{i=1}^m(x_1-x_1^{(i)})^2+\sum_{i=1}^m(x_2-x_2^{(i)})^2+\sum_{i=1}^m(x_3-x_3^{(i)})^2+\dots+\sum_{i=1}^m(x_n-x_n^{(i)})^2$$
从这里也可以看出来，其实不过多少维的数据集，$x_i$与$x_j$之间都是相互独立的(都是整体的一个基向量)整体的最小值即研究$\sum_{k=1}^m(x-x_k)^2的最小值(一维直线上的点平方和最小)$
记函数为
$$f(x) = \sum_{k=1}^m(x-x_k)^2 = mx^2-2\sum_{k=1}^mx_kx+\sum_{k=1}^mx_k^$2$$
可见，是一个二次函数，开口向上，最小值为$x = \frac{\sum_{k=1}^mx_k}{m}$时，即质心位置

## k-means 初始化

初始迭代点的选取：
可以直接挑某几个点作为初始迭代点

![image-20221205200430134](image-20221205200430134.png)

但有时会导致局部最优值

![image-20221205200550334](image-20221205200550334.png)

最佳的还是从上述的题目中挑出来代价函数J最小的情况

![image-20221205200740389](image-20221205200740389.png)

![image-20221205201045215](image-20221205201045215.png)

方法就是初始化的所有尝试中选择J最小的先

## 集群数量K的选择

集群数量K的选择：
> 因为是无监督学习，所以没法给出一个很具体的正确答案方向，即没法告知优化方向

选择k值的一种方法-**肘部法则**-**elbow method**
手肘法
![image-20221205201615208](image-20221205201615208.png)

![image-20221205201739720](image-20221205201739720.png)

> 很多时候也是看实际上的利用价值，以及图像的压缩

# 异常检测-Anomaly detection

## 异常检测介绍

> 异常检测通过观察正常事件的未标记数据集，从而学会检测异常或者在异常事件发生时告知

测试集是否与训练集相似？（有点儿像数理统计）

![image-20221205202556729](image-20221205202556729.png)

很像假设检验-接受H0与拒绝H0

![image-20221205202910402](image-20221205202910402.png)

![image-20221205203238737](image-20221205203238737.png)

## 高斯正态分布-Gaussian distribution

> 也叫**bell shaped distribution**（钟型分布）

![image-20221205203837988](image-20221205203837988.png)

![image-20221205204054731](image-20221205204054731.png)

实际上要是只考虑单个的正态分布信息，怎么综合考量成了一个问题

![image-20221205204229313](image-20221205204229313.png)

上式实际上是极大似然估计，统计学多用无偏估计
<img src="./assets/image-20221205205800427.png" alt="image-20221205205800427" style="zoom: 53%;" />

![image-20221205205847401](image-20221205205847401.png)

嘎嘎，远古课件限时返厂

## 异常检测算法

> 基于事件的独立性，我们可以得到向量的概率值为基向量概率乘积

![image-20221206201045245](image-20221206201045245.png)

嘎嘎，数学知识普及：<img src="./assets/image-20221206201117927.png" alt="image-20221206201117927" style="zoom:50%;" />

![image-20221206201229355](image-20221206201229355.png)

![image-20221206201306005](image-20221206201306005.png)

![image-20221206201429307](image-20221206201429307.png)

概率过小就是异常现象，即小概率事件

> 这里实际上是假设各个特征独立，所以直接相乘就是分布函数，但是一般会是多元正态分布

![image-20221206201935169](image-20221206201935169.png)

## 异常检测的设计与评估

![image-20221206203636844](image-20221206203636844.png)

实际上之事告诉了一个划分标准，并没有告诉数据集里哪些坏哪些好

![image-20221206204044884](image-20221206204044884.png)

![image-20221206204330398](image-20221206204330398.png)

数据倾斜是如何改正修改

> 实际上这里已经很像0，1分类监督学习了
> 但是还是有区别的，看下面监督学习与异常检测 对比
> logistic函数
> [[第3章-逻辑回归和正则化#sigmoid（logistic）function -激活函数]]

# 算法选择技巧

## 监督学习vs异常检测

- 监督学习：数据错的跟对的都很多，大概**五五开**
- 异常检测：数据大部分都很正常，极少部分异常

![image-20221206230201357](image-20221206230201357.png)

> 例子：Fraud-金融诈骗：有很多的方法，每年每月都有很多新的诈骗形式，异常检测就可以找到与之前不同的异类。Spam-垃圾邮件：但是监督学习是学习之前的不好的案例，然后接着找不好的。In short：异常检测能检测出新错误（新情况），监督学习是根据之前的给你分类（只是搬运工）。

**一个关注过去，一个着眼未来**

![image-20221206231201557](image-20221206231201557.png)

## 特征的选择

> 异常检测肯定得好好选，要不然有一个不是高斯分布，会影响整个的分布

尝试去转化一些特征的分布(数据的预处理)

![image-20221206231716648](image-20221206231716648.png)

![image-20221206231826276](image-20221206231826276.png)

<img src="./assets/image-20221206231837999.png" alt="image-20221206231837999" style="zoom:67%;" /><img src="./assets/image-20221206231853396.png" alt="image-20221206231853396" style="zoom:67%;" /><img src="./assets/image-20221206231945809.png" alt="image-20221206231945809" style="zoom:67%;" />

上面是几种不同的转换（有的ln会出现0，我们+0.000001**加小值**即可）

记得对cv与test数据集也转换

![image-20221206232325331](image-20221206232325331.png)

出现一些判断错误时，可以单看同一概率下其余数据的具体特征，添加或者删除某些不正常的点

![image-20221206232634231](image-20221206232634231.png)

![image-20221206232650159](image-20221206232650159.png)

![image-20221206232716657](image-20221206232716657.png)

如果正常是大-大，小-小，那么可以建立一个新的特征是二者的比-就会出现异常值

![image-20221206233008822](image-20221206233008822.png)
