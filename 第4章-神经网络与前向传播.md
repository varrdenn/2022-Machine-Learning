# WEEK 4



[TOC]

# 开篇

**Neural Networks**-神经网络

**inference**-推理

# 神经网络-Neural Networks

> 超级简化的人类大脑

## 神经网络介绍

![image-20221126135716626|520](image-20221126135716626.png)

NLP-自然语言处理

![image-20221126135834563|320](image-20221126135834563.png)

**axon**-轴突，**dendrites**-树突

<img src="./assets/image-20221126140043746.png" alt="image-20221126140043746" style="zoom:67%;" />

大脑是如何工作的尤为重要，出发点是生物原理，但是更重要的还是工程原理，我们怎么有效的工作。利用神经网络可以利用更多的数据。

<img src="./assets/image-20221126140421300.png" alt="image-20221126140421300" style="zoom:67%;" />

## 需求预测-Demand Prediction

**activation**-激活，**neuron**-神经元，**awarenss**-知名度

用神经元输出估价的概率

<img src="./assets/image-20221126141335411.png" alt="image-20221126141335411" style="zoom:67%;" />

<img src="./assets/image-20221126141433579.png" alt="image-20221126141433579" style="zoom:67%;" />

实际上下一层可以读取上一层的所有数据，每层输入一个数据，输出一个数据

![image-20221126141623723](image-20221126141623723.png)

**input layer** 输入层（layer0）-**hidden layer** 隐藏层（layer1）-**output layer **输出层（layer2）

> 输入层一般不记，故上述的只是一个==2层网络==

神经网络不用我们自己处理数据特征，可以借助网络自己生成合理的权重。

![image-20221126142330811](image-20221126142330811.png)

**architecture**-结构，:D

## Example:视觉处理-Face recognition

<img src="./assets/image-20221126142632734.png" alt="image-20221126142632734" style="zoom:67%;" />

第一层：找各种直线；第二层：检测脸部部位；第三层：组成人脸；实际上是从小到大的区域范围搜索（不过都是机器自己学习）

![image-20221126143014351](image-20221126143014351.png)

不仅仅是人脸，这套系统会自动小范围到大范围找到合适的分类（比如对汽车）

![image-20221126143117788](image-20221126143117788.png)

## 神经网络层-Neural network layer

1. 第一步，计算layber1

![image-20221126150744743](image-20221126150744743.png)

【i】的数据输入到【i+1】

2. 第二步，计算layber2

![image-20221126151011590](image-20221126151011590.png)

3. 第三步，对结果进行判断

![image-20221126151118262](image-20221126151118262.png)

## 更加复杂的神经网络

![image-20221126152545587](image-20221126152545587.png)

信息输出与传递的方程（第一层就是$a^{[0]}$）

![image-20221126152804101](image-20221126152804101.png)

# 推理-Inferencing

## 向前传播(手写识别)-forward propagation

![image-20221126153213153](image-20221126153213153.png)

<img src="./assets/image-20221126153223952.png" alt="image-20221126153223952"  /><img src="./assets/image-20221126153254504.png" alt="image-20221126153254504" style="zoom: 50%;" />

forward propagation-向前传播，传播激活信号

## 代码部分-Inferencing in code

>咖啡豆问题-coffee roasting

![image-20221126153817325](image-20221126153817325.png)

![image-20221126154035350](image-20221126154035350.png)

要加载各个参数以及库的提取应用。

>手写识别问题

![image-20221126154213907](image-20221126154213907.png)

## Tensorflow框架

### Numpy中的矩阵数据表达

![image-20221126155415332](image-20221126155415332.png)

**row**-行，**column**-列

![image-20221126155648808](image-20221126155648808.png)

行向量
列向量
最后一个表示一组数据，一个列表
### TensorFlow中的矩阵数据表达
**tensor**-张量，用来储存数据。张量可以视为高维矩阵的储存方式（用长方体储存数据）

![image-20221126160548039](image-20221126160548039.png)

> 这里最后一行应该是弄错了,应该都是0.2,07,0.3。证据如下：

![image-20221126161108371](image-20221126161108371.png)

# 构造一个神经网络

## 调用库写函数

model中sequential用来给layer排序。然后输入数据去训练

![image-20221126161808516](image-20221126161808516.png)

![image-20221126161928839](image-20221126161928839.png)

一遍一遍的驯化这个函数

![image-20221126161936289](image-20221126161936289.png)

这就体现了model库的**方便**，但是还是要实际上理解库中的东西，要了解算法怎么工作的。

## 神经网络单层中的传播方向


**前向传播**（Forward Propagation）前向传播就是从input，经过一层层的layer，不断计算每一层的z和a，最后得到输出y^ 的过程，计算出了y^，就可以根据它和真实值y的差别来计算损失（loss）。
**反向传播**（Backward Propagation）反向传播就是根据损失函数L(y^,y)来反方向地计算每一层的z、a、w、b的偏导数（梯度），从而更新参数。
二者区别：[前向传播（forward）和反向传播（backward）](https://zhuanlan.zhihu.com/p/447113449)

当输入神经元没有被激活，或者输出神经元处于饱和状态，权重和偏置会学习的非常慢，这不是我们想要的效果。这也说明了为什么我们平时总是说**激活函数**的选择非常重要。

前向传播：不断迭代代价函数
反向传播：总共只需2次前向传播的计算量

![image-20221126162643104](image-20221126162643104.png)

>这里是先算的下面再算的上面
>w2_1中应该是**三维向量**，可以改成([-7，8，9])
>因为a1=(1,0,1)

**实现方法**-Tensorflow算是一种张量计算的架构

定义函数，输入上一层参数，系数阵，得到下一层参数。

![image-20221126170505427](image-20221126170505427.png)

> 实际上这里应该用矩阵（向量）相乘更好-a_in乘W+b
>
> 会根据代价函数**一步一步往前修改参数**（反向传播）



# AGI猜想-Is there a path to AGI

**ANI**-Artificial General Intelligence-强人工智能，**AGI**-Artificial Narrow Intelligence-弱人工智能

![image-20221126171542714](image-20221126171542714.png)

我们想的可能越多的neuron，就能模拟人脑，但是还是太简单，未能达到模拟人脑的程度。

"one learning algorithm" hypothesis-简单算法猜想

听觉的切断把视觉输入，会自动接受视觉（就像有一个算法，我们的算法“大脑”接收数据并生成结果）

![image-20221126171944549](image-20221126171944549.png)

![image-20221126172133997](image-20221126172133997.png)

生物具有惊人的适应性，那么能够复制这些算法吗。

# 向量化求解-矩阵部分

向量化的成就铸就了今天的深度学习

![image-20221126183557103](image-20221126183557103.png)

看是否大于0.5取得0，1所以最后输出的都是0.1

dot product-点积（点乘），transpose-转置

![image-20221126183926985](image-20221126183926985.png)

![image-20221126190058163](image-20221126190058163.png)

<img src="./assets/image-20221126190412100.png" alt="image-20221126190412100" style="zoom:80%;" />

<img src="./assets/image-20221126190941872.png" alt="image-20221126190941872" style="zoom:80%;" />

![image-20221126191109917](image-20221126191109917.png)

向量化复杂，但是很值

# 神经网络向量化实现

<img src="./assets/image-20221126193230962.png" alt="image-20221126193230962" style="zoom:67%;" />

![image-20221126193734282](image-20221126193734282.png)

直接用矩阵乘积进行传播。

