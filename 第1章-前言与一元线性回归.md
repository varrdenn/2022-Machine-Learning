# WEEK 1

[TOC]
[[第1章-前言与一元线性回归#^ezbcgd]]
# 前言

吴佬星战迷（全视频出现了三次）

![image-20221208211206978](image-20221208211206978.png)

谷歌的搜索，邮件的过滤，都是机器学习。最想要的就是用机器模拟人的思维。

> 没有明确编码的机器解决问题的方法，让机器自己去学习Artifical General Inteligence New capability

- **人工手写代码**：数据挖掘，医学记录，工程
- **无法写代码**：自然语言处理，手写识别
- **推荐系统**：各种推广与学习

下面是吴佬的百宝袋

<img src="./assets/image-20221208211457647.png" alt="image-20221208211457647" style="zoom:50%;" /><img src="./assets/image-20221130201750256.png" alt="image-20221130201750256" style="zoom: 50%;" /><img src="./assets/image-20221208162459566.png" alt="image-20221208162459566" style="zoom:50%;" /><img src="./assets/image-20221208193927019.png" alt="image-20221208193927019" style="zoom: 50%;" /><img src="./assets/image-20221208194248880.png" alt="image-20221208194248880" style="zoom: 67%;" />

# 机器学习-定义

![7e711d90b9d94f2fb1b8c8c8c77db0bb](7e711d90b9d94f2fb1b8c8c8c77db0bb.png)

![c244ad6a9c4c4da0a0361e659c05a523](c244ad6a9c4c4da0a0361e659c05a523.png)

>**T**指机器所能帮助人解决的问题
>**P**指做到的程度（性能参数）
>**E**是指人们的输入实践

![5e196e68393b4d0bb3a6bc9f8bccfcc0](5e196e68393b4d0bb3a6bc9f8bccfcc0.png)
## 算法学习分类
>监督学习：1-3章
>高级算法：4-7章
>无监督学习：8-10章
## 监督学习

![1a03f34c700c4da5800202f69208acc1](1a03f34c700c4da5800202f69208acc1.png)

> 比如一个估值问题，用直线与曲线，给他我们的定义与数据，让机器去找到合适的解

![668d47a6d2da4e278776538a8b3b876a](668d47a6d2da4e278776538a8b3b876a.png)

### 回归问题-regression problem

![eed429a4ba5f4cffbcadf83e67503b0a](eed429a4ba5f4cffbcadf83e67503b0a.png)

>也叫做回归问题，用正确的数据推测预测的数据。从无限的数据中找到合适的那一个。

### 分类问题-classification problem

![f557b1d5e1c44b49a6823de802c9eabd](f557b1d5e1c44b49a6823de802c9eabd.png)

分类问题，**01分类**，还有更多种分类，不一定是具体数值，可能是判断是猫是狗还是。。。

![cd133f90f05749bf845c1189c2d16eac](cd133f90f05749bf845c1189c2d16eac.png)

另一种表达方式

![9394a22f77ce4c8eb11fb53edbea3b3e](9394a22f77ce4c8eb11fb53edbea3b3e.png)



## 无监督学习

![780ed8fb4328435aaee4f703f2bf50ab](780ed8fb4328435aaee4f703f2bf50ab.png)

### 聚类-**Custering**
>不再给出问题的具体分类信息，让机器自己去分类
>如：kmeans分类问题（maybe，多轴就要涉及到多个特征甚至无穷特征，计算机如何存储——**向量机**。 2025-01-27T16:09:49+08:00
![370a076a19674e74a26ba6129b28f03b](370a076a19674e74a26ba6129b28f03b.png)

![e0130f4c7d4c43758fc9027664e38886](e0130f4c7d4c43758fc9027664e38886.png)

算法识别叠加声音，分离音频

![f203314bdf2b48cfbfb629363091f214](f203314bdf2b48cfbfb629363091f214.png)

## 两者的区别

![edee2f7bcc79448fbf32413ce9fe1171](edee2f7bcc79448fbf32413ce9fe1171.png)

# 一元线性回归-linear regression with one variable

## 前言-线性回归的介绍

<img src="./assets/55dcc376be8e40a6b1b8cf2a1b8f6806.png" alt="ab8f843009e24a338b31d366b289b3e8.png" style="zoom: 80%;" />

<img src="./assets/image-20221123142827819.png" alt="image-20221123142827819" style="zoom: 50%;" />

**hypothesis function-假设函数**

<img src="./assets/image-20221123143120306.png" alt="image-20221123143120306" style="zoom:50%;" /><img src="./assets/155ab9e777fc478bbbbc1d69c3b740d6.png" alt="5b4196e408931f258761dc559f11b80a.png"  />

**linear regression-线性回归**(with one variable)下图为多种情况下的回归直线

![image-20221123143905555](image-20221123143905555.png)

## 代价函数J-Cost Function

> 注意，这个函数是指的残差平均值与代价函数参数的关系

![image-20221123144755359](image-20221123144755359.png)

univariate linear regression(单变量线性回归)
$$
h_{\theta}(x) = \theta_{0} + \theta_{1}x\\
 后面改版的\quad f_{w,b}(x) = wx+b
$$

线性回归目标(parameters-参数)
$$
min \frac{1}{2m}\sum_{i = 1}^m (f_{w,b}(x^{(i)}) - y^{(i)} )^2
$$

![3ee467ef6c8ca8175c5e613ab1be5932.png](edd6a50d03ad47eb9dc17cd9e70e9df1.png)

> 至于为什么要除以2m，可以把这个min函数视为一个二元函数f(o1,o2)，所以在求最小时要求偏导求极值点，会产生2，所以除以2以消去。至于除以m，就是求的平均数。

**training set**训练集

![0fc4764ef9657fc60dde6afb59806424.png](882c2999d986496fbe76cb24913278f0.png)

训练集为（1，1）（2，2）（3，3）这三个点

![1640faa8710b804c6db281d8fe30af58.png](37cafdb5d6a44e49afa753bb32511f11.png)

![669ca76f29ba3fea66647770f597518a.png](4276973bf59840308b366ee1118c190f.png)

![732febc24fad75824556ad3e9ce6ba97.png](bd9ae92971254af4a96304cd1315143e.png)

> 最小二乘法实际上就是用偏导数为0推导出来的极值点

**contour plots** or **contor figures**等高线图，更加直观的表示三维图像（二维变量下的代价函数）。

<img src="./assets/image-20221123161437526.png" alt="image-20221123161437526" style="zoom: 50%;" />

<img src="./assets/image-20221123161819918.png" alt="image-20221123161819918" style="zoom:67%;" />

![0221d92852b5975259a82131bac22ead.png](55f495cdce81400894d49d6596b4b558.png)

> 怎么设计来找到自动的计算程序，以及多维难以可视化的数据集。

## 梯度下降-Gradient descent algorithm

二元的线性回归最终都会是碗状结构。
 <img src="./assets/526bc0e47ea748ce962c56d7a4ffe948.png" alt="c5c24615f475f6c06bb5878ddf3d2c87.png" style="zoom:67%;" />

 <img src="./assets/1acd90e41ab9410c87858b6e7f0096a8.png" alt="8251084b3656b5d6b84e9940101a2d72.png" style="zoom:67%;" />

 梯度下降，实际上就是环顾一周，找到一个下降速度最快的方向——little baby step

 ![2f3a5700096d038fd31f3d199ad6aaf5.png](bf90f26fba95429bbb33d4740b000c9b.png)

 >**=**代表赋值，一个很好的计算机与数学的区分方式。

<img src="./assets/image-20221123164301647.png" alt="image-20221123164301647" style="zoom:67%;" />

**注意**要同时更新两个参数

![605ef743bebb0f541c36490754ad2275.png](f82506afb35243c4ab7c4b65e613e31b.png)

 $\alpha$记作是**learning rate 学习速率**

 对于怎样理解这个梯度下降算法，可以用仅一个参数的情况分析。 
<img src="./assets/67460cc8af7b4849851817ba2d0bb88f.png" alt="b2f978363e65b48f5801ed1482b8ea2e.png" style="zoom:67%;" />

 - 在左侧，导数小于0，$\theta$会变大
 - 在左侧，导数大于0，$\theta$会变小
 - 实际上就是在向最小值点靠近（derivative-导数）

<img src="./assets/60d3076afc284861a13ab3a94a7eb483.png" alt="fa57c2c0060374785cbdd509c05f7004.png" style="zoom:67%;" />
而对于$\alpha$的取值，过小会**变慢**，过大会**发散**（变化大小还与导数大小-斜率有关）在数值计算方法中，指出了$\alpha$也可以随着迭代发生变化（最后一步应该是0)

<img src="./assets/image-20221123171839182.png" alt="image-20221123171839182" style="zoom:67%;" />

所以说，降到了最低点之后就不在下降了

<img src="./assets/image-20221123172122015.png" alt="image-20221123172122015" style="zoom: 67%;" />

## 算法的实现

<img src="./assets/image-20221123172324201.png" alt="image-20221123172324201" style="zoom:80%;" />
$$
实际上就是\frac{d}{dw}J(w,b) = 2\times J(w,b) \times \frac{J(w,b)}{w}
$$
<img src="./assets/image-20221123172612693.png" alt="image-20221123172612693" style="zoom:80%;" />

二元线性回归中只有一个极小值（全局最小）**convex function**（凸函数）

<img src="./assets/image-20221123173023832.png" alt="image-20221123173023832" style="zoom: 80%;" />

![image-20221123173225852](image-20221123173225852.png)

## 算法实现

自己实现以下算法

```matlab
生成一个随机的训练集
clc;clear;clf;
train_set = 10*rand(1000,2);
x0 = train_set(:,1);
y0 = train_set(:,2);
scatter(x0,y0)

首先绘制一下迭代等高线图
n = 100;
k = linspace(0.01,0.03,n);
b = linspace(5,7,n);
[kk,bb]=meshgrid(k,b);
z = 0;
for i = 1:length(x0)
    z = z + ( kk.*x0(i)+bb-y0(i)*ones(n,n) ).^2;
end
z = z/2/length(x0);
min(min(z))
contour(kk,bb,z);

然后我们试验一下梯度下降方法
%给出初始值
k = 0;%初始值随便改
b = 0;%初始值随便改
a = 0.04;%换称0.1就发散了，乐
figure
contour(kk,bb,z)
hold on
for i = 1:1000
    k1 = 0;
    k2 = 0;
    for j = 1:length(x0)
        k1 = k1 + ( k*x0(j) + b - y0(j) ) * x0(j);
    end
    for j = 1:length(x0)
        k2 = k2 + ( k*x0(j) + b - y0(j) );
    end
    plot(k,b,'o')
    k = k - a * k1 / length(x0);
    b = b - a * k2 / length(x0);
end

看看是否收敛
hold off
k = 0;%初始值随便改
b = 0;%初始值随便改
a = 0.04;%换称0.1就发散了，乐
n = 100000;
x00 = linspace(0,10,100);
kk = zeros(1,n);
bb = zeros(1,n);
for i = 1:n
    k1 = 0;
    k2 = 0;
    for j = 1:length(x0)
        k1 = k1 + ( k*x0(j) + b - y0(j) ) * x0(j);
    end
    for j = 1:length(x0)
        k2 = k2 + ( k*x0(j) + b - y0(j) );
    end
    k = k - a * k1 / length(x0);
    b = b - a * k2 / length(x0);
    kk(i) = k;
    bb(i) = b;
end
xx = 1:n;
z = 0;
for i = 1:length(x0)
    z = z + ( kk(n)*x0(i)+bb(n)-y0(i) )^2;
end
z = z/2/length(x0)
plot(xx,kk,xx,bb)
%到后面就收敛了，不过迭代次数有点儿多

```

其实在自己实现算法的过程中，发现了其实最难的**初始点**与**学习速率**的挑选，很容易就会发散