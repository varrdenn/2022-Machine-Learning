# WEEK 7

[TOC]

# 决策树 Decision Tree Model

## 猫猫分类案例-可爱捏

![image-20221130163246924](image-20221130163246924.png)

跟绿萝一样，根在上面，叶子在下面

**root node**-根部节点，**decision nodes**-决策节点，**leaf nodes**-叶节点

![image-20221130163622351](image-20221130163622351.png)

![image-20221130163715903](image-20221130163715903.png)

## 学习过程-learning process



先按耳朵形状分类

![bili_v_d_1669797991515](bili_v_d_1669797991515.gif)

然后按照脸型分类

![bili_v_d_1669797954142](bili_v_d_1669797954142.gif)

当样本足够纯（全是猫or全不是猫）时，添加叶节点

![image-20221130165212008](image-20221130165212008.png)

1. 怎么确定用什么特征来划分![image-20221130165402633](image-20221130165402633.png)
2. 何时停止分类（很纯的时候or有限制深度or纯度增加太小时or精度达到时）![image-20221130170034169](image-20221130170034169.png)

## 熵的引入

entropy-熵，数值越大越不纯

![image-20221130173551384](image-20221130173551384.png)
$$
显然，p_{a}=1-p_{1-a}
$$
![image-20221130174226135](image-20221130174226135.png)

实际上之前的那个代价函数就是交叉熵，算是这个的一种
如：
二元分类logistic：[[第3章-逻辑回归和正则化#代价函数L的理解：交叉熵]]
多类问题softmax：[[第5章-神经网络实现（激活函数应用）#softmax的代价函数]]
## 选择拆分信息增益

![image-20221130185426523](image-20221130185426523.png)

我们比较希望出现小的，因为越接近0，他的内容就越纯（全是猫，或全是狗）我们就可以给他添加叶节点了。我们可以看熵的变化-**information gain**，下一层熵的值用加权平均计算。

![image-20221130191616051](image-20221130191616051.png)

下降多的表现更好

![image-20221130192109443](image-20221130192109443.png)

## 整合

![image-20221130192318237](image-20221130192318237.png)

从左到右依次加入leaf node-叶节点

![image-20221130193713311](image-20221130193713311.png)

![image-20221130193655320](image-20221130193655320.png)

**recursive algorithm**-递归算法-不断地建立小的决策树

![image-20221130193947212](image-20221130193947212.png)

有很多方法来决定何时停止，实际上也有很多的开源库可以利用。

# 分类特征

## 独热-one hot

> 上述我们的那些都是二元值，而我们下面用的是多元的编码

![image-20221130194415595](image-20221130194415595.png)

二进制的表达方式（实际计算时就是按照1.0划分）

![image-20221130194511048](image-20221130194511048.png)

**one-hot encoding**-独热编码，这样就可以取更多的离散特征值

![image-20221130194717558](image-20221130194717558.png)

## 连续值的特征

![image-20221130195025091](image-20221130195025091.png)

选择信息增益最好的那一个中间值

![image-20221130195532338](image-20221130195532338.png)

# 推广

## 回归树-regression trees

图中是求的平均值

![image-20221130195948849](image-20221130195948849.png)

（此时就不考虑熵的变化，而是要考虑减少方差了）计算加权方差

![image-20221130200603942](image-20221130200603942.png)

## 集成树-tree ensembles

> 单个的树对信息的敏感性很高

![image-20221130201211151](image-20221130201211151.png)

这样的话，整体的树就不会对某个数据过于敏感

![image-20221130201407343](image-20221130201407343.png)

## 有放回抽样-sampling with replacement

replacement-放回

![image-20221130201750256](image-20221130201750256.png)

![image-20221130201745798](image-20221130201745798.png)

![image-20221130201857506](image-20221130201857506.png)

## 随机森林法

> 非常强大的一种树模型，

![image-20221130202749015](image-20221130202749015.png)

最后让这些树群进行投票，往往更加有效，相对于单个树来说（其实就是为了快而增大训练量）

为了让树与树之间差异较大（要是都一样那也没意思）所以特征选择一般是n个的元素个数为k的子集，然后以这k个特征来训练(k一般取$\sqrt{n}$)

![image-20221130203413900](image-20221130203413900.png)

## XGBoost-eXtreme Gradient Boosting

>  决策树集成方法-XGBoost

![image-20221130203900551](image-20221130203900551.png)

更多的选择让之前决策错误的例子（多样性训练集）-这样就像可以练习，每次都练习自己不熟悉的部分

![image-20221130204336833](image-20221130204336833.png)

下面介绍这个开源的算法的优点

![image-20221130204443310](image-20221130204443310.png)

![image-20221130204529467](image-20221130204529467.png)

# 什么时候用决策树

> 表格数据（不建议用在图像，音频上）

![image-20221130205219774](image-20221130205219774.png)

神经网络中更多的调用连续函数，更加方便梯度下降训练

