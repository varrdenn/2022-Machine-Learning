# WEEK 3

[TOC]

# 逻辑回归-Logistic regression

> 对于模型的输出-“y的值”，只对2元分类问题的中间过渡部分起作用，其他的具体形状之类的还是要用其他函数构成复合函数

**binary classification**-二元分类问题（class与category对等，即都代表了表示的结果类型）

逻辑回归指用0.1逻辑值表示输出（是，否；对，错；这边，那边。。。）

<img src="./assets/image-20221124173730072.png" alt="image-20221124173730072" style="zoom:67%;" />

在分类的问题中，线性模型的线性性质会影响一定的判断。（添加了一个误差较大的值之后线性模型变化较大）

![image-20221124175437504](image-20221124175437504.png)

## **sigmoid（logistic）function**-激活函数
二者区别：
Sigmoid函数和Logistic函数的关系可总结如下：

1. **定义范围**：
   - **Sigmoid函数**：泛指所有具有S形曲线的函数，如Logistic函数、双曲正切（tanh）、反正切函数等。
   - **Logistic函数**：特指形式为 $( f(x) = \frac{1}{1 + e^{-x}} )$ 的函数，是Sigmoid函数的一种具体实现。

2. **数学特性**：
   - Logistic函数将输入映射到(0,1)区间，常用于表示概率（如逻辑回归）。
   - 其他Sigmoid函数可能具有不同的输出范围，例如tanh的输出为(-1,1)。

3. **应用场景**：
   - 在机器学习中，“Sigmoid”通常默认指Logistic函数，尤其在逻辑回归和神经网络激活函数的上下文中。
   - 在数学或统计学中，Sigmoid可能泛指更广泛的S形函数，需结合上下文判断具体含义。

4. **术语使用**：
   - 严格来说，Logistic函数是Sigmoid函数的一个子类。
   - 在非正式场合（如机器学习文献），两者常被当作同义词使用，但需注意可能存在其他S形函数。

**结论**：  
Logistic函数是Sigmoid函数的一个特例，而Sigmoid函数是一类S形函数的统称。在机器学习领域，两者常被混用，但理解其区别有助于避免概念混淆。
![image-20221124204410338](image-20221124204410338.png)

![image-20221124204620190](image-20221124204620190.png)

考虑到概率问题

![image-20221124204849263](image-20221124204849263.png)

## 决策边界

![image-20221124210034389](image-20221124210034389.png)

![image-20221124210005155](image-20221124210005155.png)

逻辑函数都是带入到这个g函数中，这是我们预测的边界。
激活函数：[[第5章-神经网络实现（激活函数应用）#激活函数]]
[深度学习笔记：如何理解激活函数？（附常用激活函数）-知乎](https://zhuanlan.zhihu.com/p/364620596)

![image-20221124210605477](image-20221124210605477.png)

# 逻辑回归的评估公式

以前的平方差公式[[第1章-前言与一元线性回归#代价函数J-Cost Function]]
对于逻辑回归来说并不是一个很好的评估公式

![image-20221124212739237](image-20221124212739237.png)

因为有0.1所以会有一些急剧的变化，此时不好取极小值，有很多局部极小值。

**逻辑回归的代价函数**
## 代价函数L-Cost function for logistic regression
![image-20221124213514027](image-20221124213514027.png)

![image-20221124213728482](image-20221124213728482.png)

![image-20221124213848958](image-20221124213848958.png)

==最后会是一个**凸函数**（证明）==

以及为什么要用这个函数作为代价函数
## 代价函数L的理解：交叉熵
[损失函数：交叉熵详解 - 知乎](https://zhuanlan.zhihu.com/p/115277553)

> 总的来说就是信息熵，使得你的成本预算与实际价值成正比，上面是相当于交叉熵的形式，之不过取0.1的时候有一个项分别被删去了，交叉熵就表现出了你这个模型与实际模型之间的分布差异

例如，假设0和01都是码字，那么我们就不清楚编码字符串0100111的第一个码字是什么，因为可能是0，也可能是01。 我们想要的属性是，任何码字都不应该是另一个码字的前缀。 这称为**前缀属性**，遵守该属性的编码称为**前缀编码**。（信息传输的过程中会一起发送）
二元分类

![image-20221125104950489](image-20221125104950489.png)

![image-20221125105115176](image-20221125105115176.png)

实际上就是极大似然估计的形式表达（使得两个分布相同概率最大的参数选取）**极大似然函数**

![image-20221125111439017](image-20221125111439017.png)

## 代价函数L的梯度下降
关于导数的**推导过程**

$$\begin{aligned}J(\vec{w},b)=-\frac{1}{m}\sum_{i=1}^m[y^{(i)}ln(f_{\vec{w},b}(\vec{x}^{(i)}))+(1-y^{(i)})ln(1-f_{\vec{w},b}(\vec{x}^{(i)}))]\\=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}ln(\frac{1}{1+e^{-(\vec{w}x+b)}})+(1-y^{(i)})ln(\frac{e^{-(\vec{w}x+b)}}{1+e^{-(\vec{w}x+b)}})]\\=\frac{1}{m}\sum_{i=1}^m[y^{(i)}ln(1+e^{-(\vec{w}x+b)})+(1-y^{(i)})ln(e^{\vec{w}x+b}+1)]\end{aligned}$$
$$\begin{aligned}\frac{\partial J(\vec{w},b)}{\partial w_j}=\frac{1}{m}\sum_{i=1}^m[y^{(i)}\frac{\partial ln(1+e^{-(\vec{w}x+b)})}{\partial w_j}+(1-y^{(i)})\frac{\partial ln(e^{\vec{w}x+b}+1)}{\partial w_j}]\\=\frac{1}{m}\sum_{i=1}^m[y^{(i)}\frac{-x_j^{(i)}e^{-(\vec{w}x+b)}}{1+e^{-(\vec{w}x+b)}}+(1-y^{(i)})\frac{x_j^{(i)}e^{\vec{w}x+b}}{1+e^{\vec{w}x+b}}]\\=\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\frac{-x_{j}^{(i)}}{1+e^{\vec{w}x+b}}+(1-y^{(i)})\frac{x_{j}^{(i)}e^{\vec{w}x+b}}{1+e^{\vec{w}x+b}}]\\=\frac{1}{m}\sum_{i=1}^m[y^{(i)}\frac{-1}{1+e^{\vec{w}x+b}}+(1-y^{(i)})\frac{e^{\vec{w}x+b}}{1+e^{\vec{w}x+b}}]x_j^{(i)}\\=\frac{1}{m}\sum_{i=1}^{m}[\frac{e^{\vec{w}x+b}}{1+e^{\vec{w}x+b}}+y^{(i)}\frac{-1-e^{\vec{w}x+b}}{1+e^{\vec{w}x+b}}]x_{j}^{(i)}\\=\frac{1}{m}\sum_{i=1}^m[\frac{e^{\vec{w}x+b}}{1+e^{\vec{w}x+b}}-y^{(i)}]x_j^{(i)}\\=\frac{1}{m}\sum_{i=1}^m[\frac{1}{1+e^{-(\vec{w}x+b)}}-y^{(i)}]x_j^{(i)}\\=\frac{1}{m}\sum_{i=1}^m[f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}]x_j^{(i)}\end{aligned}$$
$$\begin{aligned}\frac{\partial J(\vec{w},b)}{\partial b}=\frac{1}{m}\sum_{i=1}^m[y^{(i)}\frac{\partial ln(1+e^{-(\vec{w}x+b)})}{\partial b}+(1-y^{(i)})\frac{\partial ln(e^{\vec{w}x+b}+1)}{\partial b}]\\=\frac{1}{m}\sum_{i=1}^m[y^{(i)}\frac{-e^{-(\vec{w}x+b)}}{1+e^{-(\vec{w}x+b)}}+(1-y^{(i)})\frac{e^{\vec{w}x+b}}{1+e^{\vec{w}x+b}}]\\=\frac{1}{m}\sum_{i=1}^{m}[f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}]\end{aligned}$$
## 代价函数J是凸函数（证明）

$$\begin{aligned}\frac{\partial^{2}J(\vec{w},b)}{\partial w_{j}^{2}}=\frac{\partial\frac{1}{m}\sum_{i=1}^m[f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)}]x_j^{(i)}}{\partial w_j}\\=\frac{1}{m}\sum_{i=1}^m\frac{x_j^2e^{-(\vec{w}x+b)}}{(1+e^{-(\vec{w}x+b)})^2}>0\end{aligned}$$

# 欠拟合与过拟合-underfitting and overfitting

## 拟合状态介绍
> 无端瞎想：**龙格现象**
> 在科学计算领域，龙格现象（Runge）指的是对于某些函数，使用均匀节点构造高次多项式差值时，在插值区间的边缘的误差可能很大的现象。它是由Runge在研究多项式差值的误差时发现的，这一发现很重要，因为它表明，并不是插值多项式的阶数越高，效果就会越好。
 
### 欠拟合
high bias这里指欠拟合（强偏差）（性别种族特征嘎嘎）
  <img align="middle" src="./assets/image-20221125174032883.png" alt="image20221125174032883" style="zoom: 120%;" />
### 泛化
generalization(我们最希望的一个)
  <img align="middle" src="./assets/image-20221125174347366.png"
   alt="image-20221125174347366" style="zoom: 120%;" />
### 过拟合
 **overfitting**- or 过拟合 **high variance**-高方差
  <img align="middle" src="./assets/image-20221125174004001.png"
   alt="image-20221125174004001" style="zoom: 120%;" />
   一般都是过拟合，欠拟合是因为模型参数不够多，模型太简单。
 <img src="./assets/image-20221125174446897.png" alt="image-20221125174446897" tyle="zoom:157%;" />
![image-20221125174741397](image-20221125174741397.png)

## 解决过拟合（三种方法）

### **增加样本数据**

![image-20221125174946983](image-20221125174946983.png)

### **特征选择**
（减少特征数量/训练集维数）
![image-20221125175151677](image-20221125175151677.png)
### 减小参数大小-regularization正则化
  
  （不是把系数变成0，而是变成尽量小）
  
regularization正则化来解决过拟合问题
  
![image-20221125175320869](image-20221125175320869.png)

   对b的正则化影响不大，不建议对其进行改动

# 带正则化的代价函数-Cost function with regularization

## 代价函数的改进

  这里是代入计算了，比方说x取10，就有1000w_3，这里只是表示x取得很大时w惩戒系数比较大。

![image-20221125180040332](image-20221125180040332.png)

可以惩戒所有的特征（都给他变小）

![image-20221125180405372](image-20221125180405372.png)

两个权衡取最小，至于怎么取$\lambda$

![image-20221125180714793](image-20221125180714793.png)

## 线性回归的正则化

梯度下降法的变化

![image-20221125180944730](image-20221125180944730.png)

进阶技巧

![image-20221125181403220](image-20221125181403220.png)

实际上**前面的系数**说明了每次得带w都会变小一些，**后面的**是寻找最低点的迭代

## 逻辑回归的正则化

![image-20221125182248590](image-20221125182248590.png)

![image-20221125182306924](image-20221125182306924.png)
