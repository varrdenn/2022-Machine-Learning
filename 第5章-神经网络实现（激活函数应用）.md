# WEEK 5

[TOC]

# 训练神经网络

## 模型代码介绍

**SparseCategoricalCrossentropy**-稀疏分类交叉熵

![image-20221126194614902](image-20221126194614902.png)

>Step 1 构造模型
>Step 2 定义代价函数-compile（编译）
>Step 3 拟合函数

## 训练细节

**specify**-指定，列入

![image-20221126195212239](image-20221126195212239.png)

下面详细介绍神经网络的细节：**BinaryCrossentropy**-二元交叉熵函数

![image-20221126195514969](image-20221126195514969.png)

可以调用[不同的损失函数](https://zhuanlan.zhihu.com/p/44216830)such as **MeanSquaredError**-平方差

>损失函数（Loss Function ）是定义在单个样本上的，算的是一个样本的误差。  
>代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。



实际上要求偏导时在函数中都帮你做到了。

<img src="./assets/image-20221126201422170.png" style="zoom:67%;" />

（大佬们的库）

# 激活函数

## 之前讲过的一些激活函数

把很多很多的0.1函数写进整个模型：就像神经元一样。

当输入神经元没有被激活，或者输出神经元处于饱和状态，权重和偏置会学习的非常慢，这不是我们想要的效果。这也说明了为什么我们平时总是说**激活函数**的选择非常重要。

ReLU函数-**Rectified Linear Unit**(矫正线性函数)

<img src="./assets/image-20221127100107817.png" alt="image-20221127100107817" style="zoom:67%;" />

<img src="./assets/image-20221127100251025.png" alt="image-20221127100251025" style="zoom:67%;" />

## 激活函数的选择

- 二元分类问题-**Sigmoid**
- 回归问题-**Linear activation functions**线性激活函数
- 非负回归问题-**ReLU**

![image-20221127100640505](image-20221127100640505.png)

往往**Sigmoid**会使得梯度下降变慢

![image-20221127101005469](image-20221127101005469.png)

![image-20221127101056061](image-20221127101056061.png)

每年都会有新的激活函数提出

## 激活函数的重要性

> 就是不要老去用线性回归，本末倒置了

![image-20221127101328629](image-20221127101328629.png)

像这样化成两层去线性回归，还不如直接用一个线性回归。

![image-20221127101714911](image-20221127101714911.png)

每层中都用线性回归，最后用sigmoid的话，结果出来跟逻辑回归没区别

![image-20221127101903558](image-20221127101903558.png)

# 多类问题-Multiclass Problem

## 多类问题介绍

![image-20221127102210756](image-20221127102210756.png)

![image-20221127102256519](image-20221127102256519.png)

> 个人想法：把后三类分成一类，然后一步一步二分类（比较慢）

## Softmax激活函数


**embelish**-美化（泛化，推广）

![image-20221127103035206](image-20221127103035206.png)

n=2的时候
$$
a_1 = \frac{e^{z_1}}{e^{z_1}+e^{z_2}} = \frac{1}{e^{z_1-z_2}+1}\\
a_2 = \frac{e^{z_2}}{e^{z_1}+e^{z_2}} = \frac{1}{e^{z_2-z_1}+1}
$$
所以参数会有些不一样（因为之前是$z$后面的是$z_1-z_2$，所以$z = z_1-z_2$，参数上也会发生相对的变动。

![image-20221127110400750](image-20221127110400750.png)

<img src="./assets/4f42bb586827c532200fdf93d83c4d3.jpg" alt="4f42bb586827c532200fdf93d83c4d3" style="zoom: 25%;" />

是等价的，但是数值上不一定是一样的，用ln函数只要是加大惩罚力度，让$a_j$达到接近1的情况（其实另一个会趋近于oo，主要还是加快迭代速度），以使得代价函数最小。
### Softmax与logistic的联系

Softmax函数可以被看作是逻辑回归（或称作Logistic函数）在多类分类问题上的推广。在二分类问题中，逻辑回归输出一个实例属于某一类的概率，而Softmax则扩展到了多个类别，使得模型能够输出多个类别的概率预测。

![image-20221127110821574](image-20221127110821574.png)

## softmax与神经网络

特有属性，比喻要把所有的z计算出来才可以算a

![image-20221127113627875](image-20221127113627875.png)


### softmax的代价函数
**SparseCategoricalCrossentropy**-稀疏类
![image-20221127113846549](image-20221127113846549.png)

## softmax改进代码

> 怎么更加精确的计算

![image-20221127114116507](image-20221127114116507.png)

表明直接的表达式会更加的精确

![image-20221127114437950](image-20221127114437950.png)把z记为中间量而不是a

![image-20221127114805773](image-20221127114805773.png)

![image-20221127115002492](image-20221127115002492.png)

对之前的逻辑回归问题

![image-20221127115027603](image-20221127115027603.png)

# 多标签分类问题-Multi-label classification

**pedestrain**-行人

![image-20221127115309322](image-20221127115309322.png)

![image-20221127115637450](image-20221127115637450.png)

## 多类型与多标签的差异

多类型输出的是每一种值的可能性（0.1，0.2，0.7）
多标签输出的是每一个的取值（0，1，0）

# 进阶优化算法

## **Adaptive Moment estimation**算法
之前的是一个固定的learning rate，现在，我们可以找到一个更加自动化的算法**Adam**

![image-20221127141808233](image-20221127141808233.png)



![image-20221127141937129](image-20221127141937129.png)

![image-20221127142026418](image-20221127142026418.png)

**Robust**-健壮（容错更高）

# 小拓展

**dense layer type**-连接层（后一项数据由前一项给出）

![image-20221127142428981](image-20221127142428981.png)

卷积层-**convolutional layer**，卷积神经网络-**CNN**（convolutional neural network）

![image-20221127144251009](image-20221127144251009.png)

目前一些其他的前沿模型：**Transformer**,**LSTM**(长短期记忆网络）,**attention**（注意力模型）



